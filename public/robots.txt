
# robots.txt for MyToolbox - https://mytoolbox.site
User-agent: *
Allow: /

# Allow crawling of all content
Disallow: /admin/
Disallow: /private/
Disallow: /api/
Disallow: /internal/
Disallow: /tmp/
Disallow: /*.json$
Disallow: /*.xml$
Disallow: /*.txt$
Disallow: /search?*

# Crawl-delay directive
Crawl-delay: 1

# Sitemaps
Sitemap: https://mytoolbox.site/sitemap.xml

# Block specific bots that might cause issues
User-agent: YandexBot
Crawl-delay: 5

User-agent: Baiduspider
Crawl-delay: 5
